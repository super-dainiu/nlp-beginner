{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b937852-4d1b-4134-99e0-e13977baf7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ecffb5-6c91-4e5d-9883-2dc48b8ca8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embeddingFile = \"model/glove.6B.50d.txt\"\n",
    "trainFile = \"data/train.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0caa9a39-1c39-410d-bee4-352af61dba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAll(text):\n",
    "    vocab = set()\n",
    "    MAX_PADDING = 0 \n",
    "    for line in text:\n",
    "        wds = line.lower().split()\n",
    "        MAX_PADDING = max(MAX_PADDING, len(wds))\n",
    "        vocab.update(wds)\n",
    "    \n",
    "    vocab.add(\"<pad>\")\n",
    "    ivocab = {idx: w for idx, w in enumerate(vocab)}\n",
    "    vocab = {w: idx for idx, w in enumerate(vocab)}\n",
    "    return vocab, ivocab, MAX_PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1843c986-d1fa-4285-b52b-cc8c3b3a3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, vocab, MAX_PADDING):\n",
    "    vectors = []\n",
    "    for line in text:\n",
    "        line = line.lower().split()\n",
    "        line += ['<pad>'] * (MAX_PADDING - len(line))\n",
    "        vector = [vocab[word] for word in line]\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "209c059d-b07e-482c-a4c2-9de1976a78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(object):\n",
    "    def __init__(self):\n",
    "        with open(embeddingFile, 'r', encoding='utf-8') as f:\n",
    "            words = [x.rstrip().split(' ')[0] for x in f.readlines()]\n",
    "        with open(embeddingFile, 'r', encoding='utf-8') as f:\n",
    "            vectors = {}\n",
    "            for line in f:\n",
    "                vals = line.rstrip().split(' ')\n",
    "                vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
    "        \n",
    "        words.append(\"<pad>\")\n",
    "        vectors[\"<pad>\"] = None\n",
    "        vocab_size = len(words)\n",
    "        self.vocab = {w: idx for idx, w in enumerate(words)}\n",
    "        self.ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "\n",
    "        vector_dim = len(vectors[self.ivocab[0]])\n",
    "        W = np.zeros((vocab_size, vector_dim))\n",
    "        for word, v in vectors.items():\n",
    "            if word == \"<pad>\":\n",
    "                v = [0 for _ in range(vector_dim)]\n",
    "            W[self.vocab[word], :] = v\n",
    "        \n",
    "        # normalize each word vector to unit variance\n",
    "        self.W_norm = np.zeros(W.shape)\n",
    "        d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "        d[-1] = 1 # zero-divisor\n",
    "        self.W_norm = (W.T / d).T\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        return self.W_norm[self.vocab[word], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9b67429e-6b96-4aad-bcd5-6bf2bdd63232",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to GloVe \n",
    "def createEmbedding(self, target_vocab, freeze=True):\n",
    "    num_vocab = len(target_vocab)\n",
    "    num_feature = self.W_norm.shape[1]\n",
    "    W = torch.zeros((num_vocab, num_feature))\n",
    "    \n",
    "    for i, word in enumerate(target_vocab):\n",
    "        try: \n",
    "            W[i] = torch.from_numpy(glove[word])\n",
    "        except KeyError:\n",
    "            W[i] = torch.from_numpy(np.random.normal(scale=0.6, size=(num_feature)))\n",
    "        except IndexError:\n",
    "            print(word)\n",
    "    \n",
    "    emb = nn.Embedding.from_pretrained(W, freeze=freeze)\n",
    "    return emb, num_vocab, num_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f2a709d5-107b-4546-8089-ddece3677158",
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = pd.read_csv(trainFile, delimiter=\"\\t\", index_col=\"PhraseId\")\n",
    "allText = allData.Phrase\n",
    "allLabel = allData.Sentiment\n",
    "vocab, ivocab, MAX_PADDING = getAll(allText)\n",
    "allID = encode(allText, vocab, MAX_PADDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8b70231b-6a02-420c-a439-02111f7c6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train Test Split\n",
    "trainInput, testInput, trainLabel, testLabel = train_test_split(\n",
    "    allID, allLabel, test_size=0.2, random_state=42)\n",
    "\n",
    "trainInput, valInput, trainLabel, valLabel = train_test_split(\n",
    "    trainInput, trainLabel, test_size=0.25, random_state=42)\n",
    "\n",
    "trainInd = np.arange(trainLabel.shape[0])\n",
    "trainInput = torch.from_numpy(trainInput)\n",
    "trainLabel = torch.from_numpy(trainLabel.to_numpy())\n",
    "\n",
    "valInput = torch.from_numpy(valInput)\n",
    "valLabel = torch.from_numpy(valLabel.to_numpy())\n",
    "\n",
    "testInput = torch.from_numpy(testInput)\n",
    "testLabel = torch.from_numpy(testLabel.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e61875ad-5829-4304-b005-c162517655c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12310,  5433,    77,  ..., 15280, 15280, 15280],\n",
       "        [ 6349, 12023,  2664,  ..., 15280, 15280, 15280],\n",
       "        [13089, 11293, 15309,  ..., 15280, 15280, 15280],\n",
       "        ...,\n",
       "        [15431,  5346,  6349,  ..., 15280, 15280, 15280],\n",
       "        [12144,  4252,  6101,  ..., 15280, 15280, 15280],\n",
       "        [ 6349,  2752,  7883,  ..., 15280, 15280, 15280]], dtype=torch.int32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "622f516e-f78d-40f7-9863-10b74b1eca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = GloVe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4d33807e-21b5-4f53-8a74-fe5cce7f441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb, emb_dim, pad_dim, num_cls, dropout=0.5, ker_size=[3, 4, 5, 6], num_ker=[100, 100, 100, 100]):\n",
    "        super(CNN, self).__init__()\n",
    "        self.emb = emb\n",
    "        self.conv1 = nn.ModuleList([nn.Sequential(\n",
    "                                    nn.Conv1d(emb_dim, n, k),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool1d(kernel_size=pad_dim-k+1)\n",
    "                                    ) for n, k in zip(num_ker, ker_size)])\n",
    "        self.fc = nn.Linear(in_features=np.sum(num_ker),\n",
    "                            out_features=num_cls)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.emb(X)\n",
    "        X = X.permute(0, 2, 1)\n",
    "        X = [conv(X) for conv in self.conv1]\n",
    "        X = torch.cat(X, dim=1)\n",
    "        X = X.view(-1, X.size(1))\n",
    "        X = self.fc(self.dropout(X))\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "45d52ec1-16c1-4bb8-b33d-39256151784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained, num_size, num_feature = glove.createEmbedding(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a6dd26d1-25b3-43ae-9b5e-f20260924101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:29<04:21, 29.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training loss:1.3161859748966538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [01:00<04:03, 30.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training loss:1.2025967747959028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [01:31<03:34, 30.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training loss:1.1680877704884989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [02:02<03:04, 30.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, training loss:1.1423503851941454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [02:32<02:33, 30.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, training loss:1.1208051841948559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [03:04<02:04, 31.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, training loss:1.1035974214018474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [03:34<01:32, 30.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, training loss:1.0885081354810944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [04:05<01:01, 30.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training loss:1.0784949139953295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [04:36<00:30, 30.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training loss:1.0717084618998949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [05:06<00:00, 30.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training loss:1.063072437664997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(trainInd, batch_size=100, shuffle=True)\n",
    "\n",
    "model = CNN(pretrained, num_feature, MAX_PADDING, 5).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.05)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    totalLoss = 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        SELECT = batch.tolist()\n",
    "        X, y = trainInput[SELECT].to(device), trainLabel[SELECT].to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(X)\n",
    "        L = loss(logits, y)\n",
    "        totalLoss += L.item()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    totalLoss /= len(dataloader)\n",
    "    if not (i+1)%1:\n",
    "        print(f\"epoch {i+1}, training loss:{totalLoss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "094a1542-47b9-4e31-ad21-f5d23380d5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.52953992054338\n"
     ]
    }
   ],
   "source": [
    "valInput, valLabel = valInput.to(device), valLabel.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(valInput)\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    accuracy = (preds == valLabel).cpu().numpy().mean() * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5880b4-b2ae-4424-87d9-3de27279e947",
   "metadata": {},
   "source": [
    "#### ref:\n",
    "(1): https://chriskhanhtran.github.io/posts/cnn-sentence-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
