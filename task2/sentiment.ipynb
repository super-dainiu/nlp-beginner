{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b937852-4d1b-4134-99e0-e13977baf7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6ecffb5-6c91-4e5d-9883-2dc48b8ca8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embeddingFile = \"model/glove.6B.50d.txt\"\n",
    "trainFile = \"data/train.tsv\"\n",
    "models = []  # A collection of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0caa9a39-1c39-410d-bee4-352af61dba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the words and create a bijection between word and index\n",
    "def getAll(text):\n",
    "    vocab = set()\n",
    "    MAX_PADDING = 0 \n",
    "    for line in text:\n",
    "        wds = line.lower().split()\n",
    "        MAX_PADDING = max(MAX_PADDING, len(wds))\n",
    "        vocab.update(wds)\n",
    "    \n",
    "    vocab.add(\"<pad>\")  # <pad> for padding\n",
    "    ivocab = {idx: w for idx, w in enumerate(vocab)}\n",
    "    vocab = {w: idx for idx, w in enumerate(vocab)}\n",
    "    return vocab, ivocab, MAX_PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1843c986-d1fa-4285-b52b-cc8c3b3a3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence to vector\n",
    "def encode(text, vocab, MAX_PADDING):\n",
    "    vectors = []\n",
    "    for line in text:\n",
    "        line = line.lower().split()\n",
    "        line += ['<pad>'] * (MAX_PADDING - len(line))\n",
    "        vector = [vocab[word] for word in line]\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "209c059d-b07e-482c-a4c2-9de1976a78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GloVe pretrained embeddings\n",
    "class GloVe(object):\n",
    "    def __init__(self):\n",
    "        with open(embeddingFile, 'r', encoding='utf-8') as f:\n",
    "            words = [x.rstrip().split(' ')[0] for x in f.readlines()]\n",
    "        with open(embeddingFile, 'r', encoding='utf-8') as f:\n",
    "            vectors = {}\n",
    "            for line in f:\n",
    "                vals = line.rstrip().split(' ')\n",
    "                vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
    "        \n",
    "        words.append(\"<pad>\")\n",
    "        vectors[\"<pad>\"] = None\n",
    "        vocab_size = len(words)\n",
    "        self.vocab = {w: idx for idx, w in enumerate(words)}\n",
    "        self.ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "\n",
    "        vector_dim = len(vectors[self.ivocab[0]])\n",
    "        W = np.zeros((vocab_size, vector_dim))\n",
    "        for word, v in vectors.items():\n",
    "            if word == \"<pad>\":\n",
    "                v = [0 for _ in range(vector_dim)]\n",
    "            W[self.vocab[word], :] = v\n",
    "        \n",
    "        # normalize each word vector to unit variance\n",
    "        self.W_norm = np.zeros(W.shape)\n",
    "        d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "        d[-1] = 1 # zero-divisor\n",
    "        self.W_norm = (W.T / d).T\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        return self.W_norm[self.vocab[word], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b67429e-6b96-4aad-bcd5-6bf2bdd63232",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to GloVe\n",
    "# Create the embedding\n",
    "def createEmbedding(self, target_vocab, freeze=True):\n",
    "    num_vocab = len(target_vocab)\n",
    "    num_feature = self.W_norm.shape[1]\n",
    "    W = torch.zeros((num_vocab, num_feature))\n",
    "    \n",
    "    for i, word in enumerate(target_vocab):\n",
    "        try: \n",
    "            W[i] = torch.from_numpy(self[word])\n",
    "        except KeyError:\n",
    "            W[i] = torch.from_numpy(np.random.normal(scale=0.6, size=(num_feature)))\n",
    "        except IndexError:\n",
    "            print(word)\n",
    "    \n",
    "    emb = nn.Embedding.from_pretrained(W, freeze=freeze)\n",
    "    return emb, num_vocab, num_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0583336b-fb8b-4664-90a8-68583d22e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with a random embedding\n",
    "def createRandomEmbedding(target_vocab, num_feature=100):\n",
    "    num_vocab = len(target_vocab)\n",
    "    W = torch.zeros((num_vocab, num_feature))\n",
    "    \n",
    "    for i, word in enumerate(target_vocab):\n",
    "        W[i] = torch.from_numpy(np.random.normal(scale=0.6, size=(num_feature)))\n",
    "    \n",
    "    emb = nn.Embedding.from_pretrained(W)\n",
    "    return emb, num_vocab, num_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2a709d5-107b-4546-8089-ddece3677158",
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = pd.read_csv(trainFile, delimiter=\"\\t\", index_col=\"PhraseId\")\n",
    "allText = allData.Phrase\n",
    "allLabel = allData.Sentiment\n",
    "vocab, ivocab, MAX_PADDING = getAll(allText)\n",
    "allID = encode(allText, vocab, MAX_PADDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b70231b-6a02-420c-a439-02111f7c6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train Test Split\n",
    "trainInput, testInput, trainLabel, testLabel = train_test_split(\n",
    "    allID, allLabel, test_size=0.2, random_state=42)\n",
    "\n",
    "trainInput, valInput, trainLabel, valLabel = train_test_split(\n",
    "    trainInput, trainLabel, test_size=0.25, random_state=42)\n",
    "\n",
    "trainInd = np.arange(trainLabel.shape[0])\n",
    "trainInput = torch.from_numpy(trainInput)\n",
    "trainLabel = torch.from_numpy(trainLabel.to_numpy())\n",
    "\n",
    "valInput = torch.from_numpy(valInput)\n",
    "valLabel = torch.from_numpy(valLabel.to_numpy())\n",
    "\n",
    "testInput = torch.from_numpy(testInput)\n",
    "testLabel = torch.from_numpy(testLabel.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "622f516e-f78d-40f7-9863-10b74b1eca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = GloVe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888da6a-de0e-42f0-aaf6-d2846eef03ac",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d33807e-21b5-4f53-8a74-fe5cce7f441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb, emb_dim, pad_dim, num_cls, dropout=0.5, ker_size=[3, 4, 5, 6], num_ker=[100, 100, 100, 100]):\n",
    "        super(CNN, self).__init__()\n",
    "        self.emb = emb\n",
    "        self.conv1 = nn.ModuleList([nn.Sequential(\n",
    "                                    nn.Conv1d(emb_dim, n, k),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool1d(kernel_size=pad_dim-k+1)\n",
    "                                    ) for n, k in zip(num_ker, ker_size)])\n",
    "        self.fc = nn.Linear(in_features=np.sum(num_ker),\n",
    "                            out_features=num_cls)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.emb(X)\n",
    "        X = X.permute(0, 2, 1)\n",
    "        X = [conv(X) for conv in self.conv1]\n",
    "        X = torch.cat(X, dim=1)\n",
    "        X = X.view(-1, X.size(1))\n",
    "        X = self.fc(self.dropout(X))\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2434d56c-5431-4e93-8e4f-f0b954954b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, emb, emb_dim, pad_dim, num_cls, dropout=0.5, h_size=100):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.emb = emb\n",
    "        self.h_size = h_size\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim,\n",
    "                            hidden_size=h_size,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=h_size,\n",
    "                            out_features=num_cls)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        h = torch.zeros(1, X.size(0), self.h_size)\n",
    "        c = torch.zeros(1, X.size(0), self.h_size)\n",
    "        X = self.emb(X)\n",
    "        X, _ = self.lstm(X, )\n",
    "        X = X[:, -1, :]\n",
    "        X = self.fc(self.dropout(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3adb3c6-9b00-4eef-a3f8-ed3b00a4e2ed",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37fed77-2c8b-460c-885c-fde236a46f29",
   "metadata": {},
   "source": [
    "#### Random Embedding + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8628d0b5-69db-48c7-a44e-8b671e149a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      " 10%|████████▎                                                                          | 1/10 [00:36<05:25, 36.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training loss:1.2405600439649762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [01:13<04:53, 36.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training loss:1.2058526189121137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [01:51<04:20, 37.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training loss:1.1857372168670215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [02:28<03:43, 37.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, training loss:1.1696165574271244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [03:05<03:06, 37.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, training loss:1.1535215012037385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [03:43<02:30, 37.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, training loss:1.1389119273316008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [04:21<01:53, 37.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, training loss:1.1254214150165163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [04:59<01:15, 37.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training loss:1.1119742578605196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [05:37<00:37, 37.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training loss:1.0990055622006174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [06:15<00:00, 37.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training loss:1.0854281590995056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb, num_size, num_feature = createRandomEmbedding(vocab)\n",
    "dataloader = DataLoader(trainInd, batch_size=100, shuffle=True)\n",
    "\n",
    "model = CNN(emb, num_feature, MAX_PADDING, 5).to(device)\n",
    "models.append(model)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.05)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    totalLoss = 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        SELECT = batch.tolist()\n",
    "        X, y = trainInput[SELECT].to(device), trainLabel[SELECT].to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(X)\n",
    "        L = loss(logits, y)\n",
    "        totalLoss += L.item()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    totalLoss /= len(dataloader)\n",
    "    if not (i+1)%1:\n",
    "        print(f\"epoch {i+1}, training loss:{totalLoss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91561070-bf9a-49fb-9706-c8622f861b16",
   "metadata": {},
   "source": [
    "#### Untrainable GloVe Embedding + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07b36c6-d120-4e7d-bfa1-1b71c4665d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:26<04:02, 26.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training loss:1.317599594020538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [00:54<03:38, 27.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training loss:1.2019674664754878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [01:22<03:14, 27.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training loss:1.165718745332899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [01:50<02:46, 27.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, training loss:1.1423982229151364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [02:18<02:18, 27.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, training loss:1.1202839004828047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [02:45<01:50, 27.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, training loss:1.1028792397571348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [03:13<01:22, 27.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, training loss:1.0896853110579061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [03:41<00:55, 27.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training loss:1.080199436800457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [04:09<00:27, 27.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training loss:1.0725859490758962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [04:37<00:00, 27.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training loss:1.066162345121866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained, num_size, num_feature = glove.createEmbedding(vocab)\n",
    "dataloader = DataLoader(trainInd, batch_size=100, shuffle=True)\n",
    "\n",
    "model = CNN(pretrained, num_feature, MAX_PADDING, 5).to(device)\n",
    "models.append(model)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.05)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    totalLoss = 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        SELECT = batch.tolist()\n",
    "        X, y = trainInput[SELECT].to(device), trainLabel[SELECT].to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(X)\n",
    "        L = loss(logits, y)\n",
    "        totalLoss += L.item()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    totalLoss /= len(dataloader)\n",
    "    if not (i+1)%1:\n",
    "        print(f\"epoch {i+1}, training loss:{totalLoss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c5ba-150e-4cd4-aa30-5683bf56177a",
   "metadata": {},
   "source": [
    "#### Trainable GloVe Embedding + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "953c1f16-a0e2-44ab-bca5-effbf3a1caa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:47<07:07, 47.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training loss:1.2978745772719003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [01:35<06:21, 47.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training loss:1.196729633953172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [02:23<05:35, 47.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training loss:1.1609844944520338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [03:11<04:47, 47.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, training loss:1.1286219759837286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [03:59<03:59, 47.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, training loss:1.096937464357313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [04:47<03:11, 47.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, training loss:1.0673280503986484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [05:35<02:23, 47.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, training loss:1.0440866806591078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [06:23<01:36, 48.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training loss:1.024910254183294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [07:11<00:47, 47.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training loss:1.011315764268918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [07:58<00:00, 47.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training loss:0.9968385993480173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained, num_size, num_feature = glove.createEmbedding(vocab, freeze=False)\n",
    "dataloader = DataLoader(trainInd, batch_size=100, shuffle=True)\n",
    "\n",
    "model = CNN(pretrained, num_feature, MAX_PADDING, 5).to(device)\n",
    "models.append(model)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.05)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    totalLoss = 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        SELECT = batch.tolist()\n",
    "        X, y = trainInput[SELECT].to(device), trainLabel[SELECT].to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(X)\n",
    "        L = loss(logits, y)\n",
    "        totalLoss += L.item()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    totalLoss /= len(dataloader)\n",
    "    if not (i+1)%1:\n",
    "        print(f\"epoch {i+1}, training loss:{totalLoss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8a822-808d-418b-ad67-a05fad955ce6",
   "metadata": {},
   "source": [
    "#### RandomEmbedding + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7197d99-8ba8-4925-903f-1db792a83703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [01:02<09:25, 62.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training loss:1.289078432187963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [02:05<08:22, 62.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training loss:1.284036505693051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [03:08<07:20, 62.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training loss:1.2825565992068328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [04:12<06:18, 63.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, training loss:1.2824663519223192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [05:16<05:17, 63.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, training loss:1.2818492161171668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [06:20<04:15, 63.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, training loss:1.2811234588938563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [07:25<03:12, 64.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, training loss:1.2812424572453045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [08:30<02:09, 64.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training loss:1.280995098605609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [09:36<01:04, 64.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training loss:1.2806394264308723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [10:42<00:00, 64.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training loss:1.2806248742462858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb, num_size, num_feature = createRandomEmbedding(vocab)\n",
    "dataloader = DataLoader(trainInd, batch_size=100, shuffle=True)\n",
    "\n",
    "model = LSTM(emb, num_feature, MAX_PADDING, 5).to(device)\n",
    "models.append(model)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    totalLoss = 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        SELECT = batch.tolist()\n",
    "        X, y = trainInput[SELECT].to(device), trainLabel[SELECT].to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(X)\n",
    "        L = loss(logits, y)\n",
    "        totalLoss += L.item()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    totalLoss /= len(dataloader)\n",
    "    if not (i+1)%1:\n",
    "        print(f\"epoch {i+1}, training loss:{totalLoss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c278fc6-6d19-46f9-8bad-01fd4ac6ab25",
   "metadata": {},
   "source": [
    "#### Untrainable GloVe Embedding + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6dd26d1-25b3-43ae-9b5e-f20260924101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [01:00<09:06, 60.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training loss:1.2905629374555998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [02:01<08:08, 61.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training loss:1.2832786895676636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [03:03<07:09, 61.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training loss:1.282492292856076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [04:05<06:09, 61.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, training loss:1.281854307511573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [05:06<05:07, 61.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, training loss:1.2816361412803543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [06:07<04:05, 61.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, training loss:1.2813271632698329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [07:09<03:03, 61.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, training loss:1.2809435143160337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [08:10<02:02, 61.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training loss:1.2811414924477056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [09:11<01:01, 61.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training loss:1.2808794019825303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [10:12<00:00, 61.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training loss:1.280553351217044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained, num_size, num_feature = glove.createEmbedding(vocab)\n",
    "dataloader = DataLoader(trainInd, batch_size=100, shuffle=True)\n",
    "\n",
    "model = LSTM(pretrained, num_feature, MAX_PADDING, 5).to(device)\n",
    "models.append(model)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    totalLoss = 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        SELECT = batch.tolist()\n",
    "        X, y = trainInput[SELECT].to(device), trainLabel[SELECT].to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(X)\n",
    "        L = loss(logits, y)\n",
    "        totalLoss += L.item()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    totalLoss /= len(dataloader)\n",
    "    if not (i+1)%1:\n",
    "        print(f\"epoch {i+1}, training loss:{totalLoss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa16d0b-959f-4b02-9c36-fbb882eba455",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "094a1542-47b9-4e31-ad21-f5d23380d5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0; Accuracy on validation set: 55.17108804306036\n",
      "Model: 1; Accuracy on validation set: 56.455850313981806\n",
      "Model: 2; Accuracy on validation set: 59.24323977957195\n",
      "Model: 3; Accuracy on validation set: 50.88747917467641\n",
      "Model: 4; Accuracy on validation set: 50.88747917467641\n"
     ]
    }
   ],
   "source": [
    "valInput, valLabel = valInput.to(device), valLabel.to(device)\n",
    "bestAcc = 0\n",
    "bestInd = 0\n",
    "for i, model in enumerate(models):\n",
    "    with torch.no_grad():\n",
    "        logits = model(valInput)\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        accuracy = (preds == valLabel).cpu().numpy().mean() * 100\n",
    "        if bestAcc < accuracy:\n",
    "            bestAcc = accuracy\n",
    "            bestInd = i\n",
    "    print(f'Model: {i}; Accuracy on validation set: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "099dab98-e1d7-48c5-8460-b523d007d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model CNN(\n",
      "  (emb): Embedding(16532, 50)\n",
      "  (conv1): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1d(50, 100, kernel_size=(3,), stride=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool1d(kernel_size=50, stride=50, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv1d(50, 100, kernel_size=(4,), stride=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool1d(kernel_size=49, stride=49, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv1d(50, 100, kernel_size=(5,), stride=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool1d(kernel_size=48, stride=48, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv1d(50, 100, kernel_size=(6,), stride=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool1d(kernel_size=47, stride=47, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=400, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "). Accuracy on test set: 58.80430603613994\n"
     ]
    }
   ],
   "source": [
    "bestModel = models[bestInd]\n",
    "testInput, testLabel = testInput.to(device), testLabel.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = bestModel(testInput)\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    accuracy = (preds == testLabel).cpu().numpy().mean() * 100\n",
    "print(f'Best Model {bestModel}. Accuracy on test set: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a6aa386-b656-4185-8cc6-a12b377573b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      " 10%|████████▎                                                                          | 1/10 [00:27<04:07, 27.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training loss:1.322865056126451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [00:58<03:55, 29.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training loss:1.2007749759400286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [01:27<03:24, 29.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training loss:1.1623185905473845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [01:54<02:51, 28.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, training loss:1.137987034615992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [02:23<02:22, 28.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, training loss:1.1153583380939358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [02:50<01:52, 28.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, training loss:1.0970954977079288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [03:18<01:24, 28.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, training loss:1.086183250586531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [03:49<00:57, 28.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training loss:1.077152532854355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [04:17<00:28, 28.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training loss:1.0689304706381122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [04:45<00:00, 28.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training loss:1.0646403490415632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained, num_size, num_feature = glove.createEmbedding(vocab)\n",
    "dataloader = DataLoader(trainInd, batch_size=100, shuffle=True)\n",
    "\n",
    "model = CNN(pretrained, num_feature, MAX_PADDING, 5).to(device)\n",
    "models.append(model)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.05)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    totalLoss = 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        SELECT = batch.tolist()\n",
    "        X, y = trainInput[SELECT].to(device), trainLabel[SELECT].to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(X)\n",
    "        L = loss(logits, y)\n",
    "        totalLoss += L.item()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    totalLoss /= len(dataloader)\n",
    "    if not (i+1)%1:\n",
    "        print(f\"epoch {i+1}, training loss:{totalLoss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5880b4-b2ae-4424-87d9-3de27279e947",
   "metadata": {},
   "source": [
    "#### ref:\n",
    "(1): https://chriskhanhtran.github.io/posts/cnn-sentence-classification/\n",
    "\n",
    "(2): https://www.youtube.com/watch?v=0_PgWWmauHk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
