{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789d706e-e2e2-40aa-8c81-c7e4218eeef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import pprint\n",
    "import collections\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca083f0-e6f8-42a4-8620-04b37da9c54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9873da-6577-4c34-9c23-85d2f7ae2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model/\"\n",
    "train_file = \"data/snli_1.0_train.jsonl\"\n",
    "dev_file = \"data/snli_1.0_dev.jsonl\"\n",
    "embedding_file = \"model/glove.6B.50d.txt\"\n",
    "with open(train_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    train_data_raw = [json.loads(line.rstrip()) for line in lines]\n",
    "    \n",
    "with open(dev_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    dev_data_raw = [json.loads(line.rstrip()) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b107bf-3857-4eda-a849-ed43da5d82eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'annotator_labels': ['neutral'],\n",
       "  'captionID': '3416050480.jpg#4',\n",
       "  'gold_label': 'neutral',\n",
       "  'pairID': '3416050480.jpg#4r1n',\n",
       "  'sentence1': 'A person on a horse jumps over a broken down airplane.',\n",
       "  'sentence1_binary_parse': '( ( ( A person ) ( on ( a horse ) ) ) ( ( jumps ( over ( a ( broken ( down airplane ) ) ) ) ) . ) )',\n",
       "  'sentence1_parse': '(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN on) (NP (DT a) (NN horse)))) (VP (VBZ jumps) (PP (IN over) (NP (DT a) (JJ broken) (JJ down) (NN airplane)))) (. .)))',\n",
       "  'sentence2': 'A person is training his horse for a competition.',\n",
       "  'sentence2_binary_parse': '( ( A person ) ( ( is ( ( training ( his horse ) ) ( for ( a competition ) ) ) ) . ) )',\n",
       "  'sentence2_parse': '(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) (VP (VBG training) (NP (PRP$ his) (NN horse)) (PP (IN for) (NP (DT a) (NN competition))))) (. .)))'},\n",
       " 550152)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw[0], len(train_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3f79a-3677-4517-a987-8ee2aec04a10",
   "metadata": {},
   "source": [
    "**Suppse we don't need Tree LSTM, so that it's not necessary to exploit the parsing property of each sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dbdd92d-df8f-4767-bafe-4b4b916e4ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(raw, depreciated=['-']):\n",
    "    prem, hypo, label = [], [], []\n",
    "    for obj in raw:\n",
    "        # gold_label should not be '-'\n",
    "        if obj['gold_label'] in depreciated:\n",
    "            continue\n",
    "        prem.append(obj['sentence1'])\n",
    "        hypo.append(obj['sentence2'])\n",
    "        label.append(obj['gold_label'])\n",
    "    return prem, hypo, label\n",
    "\n",
    "def tokenize(lines):\n",
    "    return [line.rstrip('.').lower().split() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121608b2-7a66-4a4b-8f69-ff2dd1e273f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prem, train_hypo, train_label = load_data(train_data_raw)\n",
    "dev_prem, dev_hypo, dev_label = load_data(dev_data_raw)\n",
    "tokens = tokenize(train_prem) + tokenize(train_hypo) + tokenize(dev_prem) + tokenize(dev_hypo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2766d9e2-1736-4175-a911-a06e43a2de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane'], ['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane'], ['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane']] 1118418\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:3], len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a729d-789b-44a6-aa12-c2da9c2827f6",
   "metadata": {},
   "source": [
    "**Get the corpus and idx_to_token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83a3228e-de0d-4071-baee-0fe83a81fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = self.count_corpus_(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
    "        uniq_tokens += [token for token, freq in self.token_freqs if freq >= min_freq and token not in uniq_tokens]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        # Recursive definition\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.to_tokens(index) for index in indices]\n",
    "    \n",
    "    def count_corpus_(self, tokens):\n",
    "        if isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97938f35-b7cb-4d20-a551-ecae9633bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, vocab_truncated = Vocab(tokens, reserved_tokens=[\"<pad>\"]), Vocab(tokens, 5, reserved_tokens=[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9bcbad-6d49-4b77-8c27-24e3c1726e14",
   "metadata": {},
   "source": [
    "**Truncated corpus is necessary because many words are really sparse in this corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c859483-9d63-4472-ae20-43215f0cf2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['boys', 'glasses', ['river', ['bracelet']]],\n",
       " 44086,\n",
       " [[0, [38250], 3941], 1515])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens([100, 200, [300, [4000]]]), len(vocab), vocab[[['cvnlp', ['lol'], 'shit'], 'duck']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2363e0b2-8e73-4f22-8747-46ea04a7f211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['boys', 'glasses', ['river', ['bracelet']]], 19301, [[0, [0], 3941], 1515])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_truncated.to_tokens([100, 200, [300, [4000]]]), len(vocab_truncated), vocab_truncated[[['cvnlp', ['lol'], 'shit'], 'duck']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acea671-e58c-4801-8a90-169cf67baf13",
   "metadata": {},
   "source": [
    "**Sentence preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c30347a7-6c77-49ed-8790-30cdf4db855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, premise_tokens, hypothesis_tokens, labels, num_steps, vocab=None):\n",
    "        self.num_steps = num_steps\n",
    "        if vocab is None:\n",
    "            self.vocab = Vocab(premise_tokens + hypothesis_tokens,\n",
    "                                   min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises = self._pad(premise_tokens)\n",
    "        self.hypotheses = self._pad(hypothesis_tokens)\n",
    "        labels = list(map(lambda x: 0 if x == 'entailment' else 1 if x == 'neutral' else 2, labels))\n",
    "        self.labels = torch.tensor(labels)\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "\n",
    "    def _pad(self, lines):\n",
    "        return torch.tensor([truncate_pad(\n",
    "            self.vocab[line.split()], self.num_steps, self.vocab['<pad>'])\n",
    "                         for line in lines])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)\n",
    "    \n",
    "def truncate_pad(seq, num_steps, pad):\n",
    "    return seq[:num_steps] if len(seq) > num_steps else seq + [pad] * (num_steps - len(seq))\n",
    "    \n",
    "def load_data_snli(train_set, dev_set, vocab, batch_size=50, num_steps=50):\n",
    "    train_prem, train_hypo, train_label = train_set\n",
    "    dev_prem, dev_hypo, dev_label = dev_set\n",
    "    train_set = SNLIDataset(train_prem, train_hypo, train_label, num_steps, vocab)\n",
    "    dev_set = SNLIDataset(dev_prem, dev_hypo, dev_label, num_steps, vocab)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True)\n",
    "    dev_iter = torch.utils.data.DataLoader(dev_set, batch_size,\n",
    "                                            shuffle=False)\n",
    "    return train_iter, dev_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cba2b75a-b7fb-409a-8f47-57171a02f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9842 examples\n"
     ]
    }
   ],
   "source": [
    "train_iter, dev_iter = load_data_snli((train_prem, train_hypo, train_label),\n",
    "                                       (dev_prem, dev_hypo, dev_label),\n",
    "                                        vocab_truncated, batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a6a1d-7bc2-4bca-a261-d2e3b2012a05",
   "metadata": {},
   "source": [
    "**DataLoader seems not very efficient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ac10c36-41f0-4526-bc70-2b40680a1f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.68 s ± 139 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for prem, label in train_iter:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945a56c-4e00-4acb-9cc2-c18d2c614ab2",
   "metadata": {},
   "source": [
    "**Load GloVe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "550bf1d7-3995-4d02-9d93-0e65a1a7cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(object):\n",
    "    def __init__(self):\n",
    "        with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "            words = [x.rstrip().split(' ')[0] for x in f.readlines()]\n",
    "        with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "            vectors = {}\n",
    "            for line in f:\n",
    "                vals = line.rstrip().split(' ')\n",
    "                vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
    "        \n",
    "        vocab_size = len(words)\n",
    "        self.vocab = {w: idx for idx, w in enumerate(words)}\n",
    "        self.ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "\n",
    "        vector_dim = len(vectors[self.ivocab[0]])\n",
    "        W = np.zeros((vocab_size, vector_dim))\n",
    "        for word, v in vectors.items():\n",
    "            W[self.vocab[word], :] = v\n",
    "        \n",
    "        # normalize each word vector to unit variance\n",
    "        self.W_norm = np.zeros(W.shape)\n",
    "        d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "        self.W_norm = (W.T / d).T\n",
    "        self.num_feature = vector_dim\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        # pretrained, 0/1 (0 for pretrained, 1 for unknown)\n",
    "        if word in self.vocab:\n",
    "            return self.W_norm[self.vocab[word], :], 1\n",
    "        else:\n",
    "            return np.random.normal(scale=0.6, size=(self.W_norm.shape[1])), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e307d6-28de-482a-a915-533a87eb92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(vocab):\n",
    "    G = GloVe()\n",
    "    W = torch.zeros((len(vocab), G.num_feature))\n",
    "    select = []\n",
    "    for i in range(len(vocab)):\n",
    "        vector, pretrained = G[vocab[i]]\n",
    "        W[i] = torch.from_numpy(vector)\n",
    "        if pretrained:\n",
    "            select.append(i)\n",
    "    return nn.Embedding.from_pretrained(W, freeze=False), select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5b03094-bb22-4d9d-b03a-54304592e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, select = create_embedding(vocab_truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b3fdc72-6f29-4e74-a858-28df57549a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408e54b-2604-4033-8600-4c9287468d21",
   "metadata": {},
   "source": [
    "**We need to manually clean the gradient for pretrained part of the embeddings. Fine tunning the embedding may cause overfitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fc09339-451e-456f-b138-48765991b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(num_inputs, num_hiddens, flatten, activation='relu', dropout=0.2):\n",
    "    net = []\n",
    "    net.append(nn.Dropout(dropout))\n",
    "    net.append(nn.Linear(num_inputs, num_hiddens))\n",
    "    net.append(nn.Tanh() if activation == 'tanh' else nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    net.append(nn.Dropout(dropout))\n",
    "    net.append(nn.Linear(num_hiddens, num_hiddens))\n",
    "    net.append(nn.Tanh() if activation == 'tanh' else nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab8cab-ec2e-4ffc-8807-4d9d61a07477",
   "metadata": {},
   "source": [
    "**Attention layer basically computes the alignment of premise $\\bar{a}$ with hypothesis and the alignment of hypothesis $\\bar{b}$ with premise. If necessary [Chen et al.,2017], a BiLSTM is added to encode the sequence. In paper [Chen et al.,2017], the author indicated that no $F(\\cdot)$ is required for the output of BiLSTM, but an MLP is required for the embedded sequence in [Parikh et al. 2016].**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb52c9b3-b86f-46a8-8614-31e02908d3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attend(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers=2, f='mlp', dropout=0.2):\n",
    "        super(Attend, self).__init__()\n",
    "        if f == 'mlp':\n",
    "            self.f = mlp(num_inputs, num_hiddens, flatten=False, dropout=dropout)\n",
    "        elif f.lower() == 'bilstm':\n",
    "            self.f = nn.LSTM(num_inputs, num_hiddens, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.encoder = f\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        # f: A/B: (batch_size, seq_len, embed_size) -> A/B_bar: (batch_size, seq_len, hidden_size)\n",
    "        if self.encoder == 'mlp':\n",
    "            A_bar = self.f(A)\n",
    "            B_bar = self.f(B)\n",
    "        elif self.encoder == 'bilstm':\n",
    "            A_bar, _ = self.f(A)\n",
    "            B_bar, _ = self.f(B)\n",
    "            A, B = A_bar, B_bar\n",
    "        \n",
    "        # e: (batch_size, seqA, seqB)\n",
    "        e = torch.bmm(A_bar, B_bar.permute(0, 2, 1))\n",
    "        # A/B_tilde: (batch_size, seqB/A, embed_size)\n",
    "        A_tilde = torch.bmm(F.softmax(e, dim=-1), B)\n",
    "        B_tilde = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\n",
    "        \n",
    "        return (A, B, A_tilde, B_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0b063fb-bfd8-4160-9a2e-654c0499f848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.9337, 0.0409, 0.0598, 0.0955, 0.1283, 0.3423, 0.4160, 0.6789],\n",
       "          [0.1245, 0.8826, 0.8863, 0.4941, 0.5254, 0.9943, 0.0803, 0.9632]],\n",
       " \n",
       "         [[0.5675, 0.4124, 0.2126, 0.8347, 0.2480, 0.5631, 0.0050, 0.4898],\n",
       "          [0.1630, 0.8203, 0.5798, 0.5135, 0.1002, 0.3461, 0.8230, 0.1522]]]),\n",
       " tensor([[[0.0063, 0.6608, 0.6430, 0.7623, 0.9438, 0.8079, 0.0252, 0.2524]],\n",
       " \n",
       "         [[0.7647, 0.7510, 0.6696, 0.0868, 0.5204, 0.8029, 0.1255, 0.6940]]]),\n",
       " tensor([[[0.0063, 0.6608, 0.6430, 0.7623, 0.9438, 0.8079, 0.0252, 0.2524],\n",
       "          [0.0063, 0.6608, 0.6430, 0.7623, 0.9438, 0.8079, 0.0252, 0.2524]],\n",
       " \n",
       "         [[0.7647, 0.7510, 0.6696, 0.0868, 0.5204, 0.8029, 0.1255, 0.6940],\n",
       "          [0.7647, 0.7510, 0.6696, 0.0868, 0.5204, 0.8029, 0.1255, 0.6940]]],\n",
       "        grad_fn=<BmmBackward0>),\n",
       " tensor([[[0.5287, 0.4622, 0.4735, 0.2950, 0.3271, 0.6687, 0.2480, 0.8212]],\n",
       " \n",
       "         [[0.3707, 0.6109, 0.3913, 0.6784, 0.1761, 0.4575, 0.4030, 0.3255]]],\n",
       "        grad_fn=<BmmBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Attend(8, 4, 'mlp')\n",
    "net(torch.rand((2, 2, 8)), torch.rand((2, 1, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a5a0426-b064-4d44-84c8-cf237ef080b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0467, -0.1232, -0.0125, -0.0718, -0.2257,  0.0667, -0.0375,\n",
       "           -0.1345],\n",
       "          [ 0.0366, -0.1245, -0.0813, -0.0700, -0.1666, -0.0060, -0.0715,\n",
       "           -0.1138]],\n",
       " \n",
       "         [[ 0.0365, -0.1135, -0.0176, -0.0869, -0.2208,  0.0744, -0.0135,\n",
       "           -0.1304],\n",
       "          [ 0.0636, -0.1514, -0.0421, -0.1403, -0.1443,  0.0665, -0.0142,\n",
       "           -0.0914]]], grad_fn=<TransposeBackward0>),\n",
       " tensor([[[ 0.0558, -0.1317, -0.0351, -0.0809, -0.2062,  0.0782, -0.0124,\n",
       "           -0.1107],\n",
       "          [ 0.0677, -0.1630, -0.0480, -0.1058, -0.1464,  0.0362, -0.0006,\n",
       "           -0.0803]],\n",
       " \n",
       "         [[ 0.0492, -0.1262, -0.0253, -0.1050, -0.2260,  0.0792, -0.0168,\n",
       "           -0.1418],\n",
       "          [ 0.0429, -0.1575, -0.0861, -0.0620, -0.1583, -0.0143, -0.0479,\n",
       "           -0.1033]]], grad_fn=<TransposeBackward0>),\n",
       " tensor([[[ 0.0617, -0.1472, -0.0415, -0.0933, -0.1765,  0.0574, -0.0065,\n",
       "           -0.0956],\n",
       "          [ 0.0617, -0.1473, -0.0415, -0.0933, -0.1764,  0.0573, -0.0065,\n",
       "           -0.0956]],\n",
       " \n",
       "         [[ 0.0461, -0.1416, -0.0553, -0.0838, -0.1925,  0.0331, -0.0322,\n",
       "           -0.1228],\n",
       "          [ 0.0461, -0.1417, -0.0554, -0.0837, -0.1924,  0.0329, -0.0322,\n",
       "           -0.1227]]], grad_fn=<BmmBackward0>),\n",
       " tensor([[[ 0.0417, -0.1239, -0.0466, -0.0709, -0.1964,  0.0307, -0.0544,\n",
       "           -0.1242],\n",
       "          [ 0.0417, -0.1239, -0.0467, -0.0709, -0.1963,  0.0306, -0.0544,\n",
       "           -0.1242]],\n",
       " \n",
       "         [[ 0.0500, -0.1323, -0.0298, -0.1134, -0.1827,  0.0705, -0.0139,\n",
       "           -0.1110],\n",
       "          [ 0.0501, -0.1324, -0.0298, -0.1135, -0.1826,  0.0705, -0.0139,\n",
       "           -0.1109]]], grad_fn=<BmmBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Attend(8, 4, 2, 'bilstm')\n",
    "net(torch.rand((2, 2, 8)), torch.rand((2, 2, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc29da-389a-43fe-b41f-bd4ac570004c",
   "metadata": {},
   "source": [
    "**We would try max and avg_pooling to extract features, instead of summing the $\\{v_{ai}\\}$ and $\\{v_{bi}\\}$ together. This avoids the unexpected explosion of sentence length.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "581320bc-89c8-40c9-9cac-5552f8fd68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregate(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs):\n",
    "        super(Aggregate, self).__init__()\n",
    "        self.f = mlp(num_inputs * 4, num_hiddens, activation='relu', flatten=True)\n",
    "        self.g = mlp(num_inputs * 4, num_inputs, flatten=False)\n",
    "        self.linear = nn.Linear(num_hiddens, num_outputs)\n",
    "\n",
    "    def forward(self, A_bar, B_bar, A_tilde, B_tilde):\n",
    "        # V_A = [A_bar; A_tilde; A_bar-A_tilde; A_bar.*A_tilde]\n",
    "        V_A = self.g(torch.cat([A_bar, A_tilde, A_bar-A_tilde, torch.mul(A_bar, A_tilde)], dim=2))\n",
    "        V_B = self.g(torch.cat([B_bar, B_tilde, B_bar-B_tilde, torch.mul(B_bar, B_tilde)], dim=2))\n",
    "        \n",
    "        # max_pooling and avg_pooling. Before: (batch_size, seqA/B, embed_dim), After: (batch_size, 4 * embed_dim)\n",
    "        V_A_avg = torch.mean(V_A, dim=1)\n",
    "        V_A_max, _ = torch.max(V_A, dim=1)\n",
    "        V_B_avg = torch.mean(V_B, dim=1)\n",
    "        V_B_max, _ = torch.max(V_B, dim=1)\n",
    "        \n",
    "        # Feed the concatenation of both summarization results into an MLP\n",
    "        Y_hat = self.linear(self.f(torch.cat([V_A_avg, V_A_max, V_B_avg, V_B_max], dim=1)))\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1603c191-a6de-4bbd-a6ea-c6929c5d0ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3276, 0.4355, 0.1557],\n",
       "        [0.3291, 0.4358, 0.1519]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggr = Aggregate(8, 5, 3)  # num_input = num_hidden (of attend) * 32\n",
    "A, B, A_tilde, B_tilde = net(torch.rand((2, 2, 8)), torch.rand((2, 2, 8)))\n",
    "# A_bar.size(), A_tilde.size(), torch.mul(A_bar, A_tilde), B_bar.size()\n",
    "aggr(A, B, A_tilde, B_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba13210-a614-4500-bd05-71a3b218bb91",
   "metadata": {},
   "source": [
    "**The entire Attention network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a925744-e002-493f-a1fe-27f6eea2ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposableAttention(nn.Module):\n",
    "    def __init__(self, vocab, num_hiddens_1, num_hiddens_2, embedding=None, select=None, embed_dim=50, f='mlp'):\n",
    "        super(DecomposableAttention, self).__init__()\n",
    "        if embedding is not None:\n",
    "            self.embedding = embedding\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(len(vocab), embed_dim)\n",
    "        if select is not None:\n",
    "            self.select = select   # This is used to clear the gradients\n",
    "        else:\n",
    "            self.select = []\n",
    "\n",
    "        self.attend = Attend(embed_dim, num_hiddens_1, f=f)   # Attend(emb_dim, num_hiddens_1, num_layers=2, f='mlp')\n",
    "        if f == 'mlp':\n",
    "            self.name = 'parikh'\n",
    "            self.aggregate = Aggregate(embed_dim, num_hiddens_2, num_outputs=3) # Aggregate(embed_dim * 32, num_hiddens_2, 3)\n",
    "        else:\n",
    "            self.name = 'esim'\n",
    "            self.aggregate = Aggregate(num_hiddens_1 * 2, num_hiddens_2, num_outputs=3) # Aggregate(num_hidden_1 * 32, num_hiddens_2, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        premises, hypotheses = X\n",
    "        A = self.embedding(premises)\n",
    "        B = self.embedding(hypotheses)\n",
    "        A, B, A_tilde, B_tilde = self.attend(A, B)\n",
    "        Y_hat = self.aggregate(A, B, A_tilde, B_tilde)\n",
    "        return Y_hat\n",
    "    \n",
    "    def freeze_embedding_grad_(self):\n",
    "        self.embedding.weight.grad[self.select] = 0      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ea131-4c2d-4298-b74e-ad39525fca94",
   "metadata": {},
   "source": [
    "**In practice, we may fix the embedding for the first two epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "770b2c07-152a-4eab-b8f9-c56a371a0081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, dev_iter, loss, trainer, num_epochs, device, freeze_epoch=3, save=False, suffix=''):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    dev_acc = []\n",
    "    for i in range(num_epochs):\n",
    "        total_acc = []\n",
    "        total_loss = []\n",
    "        net.train()\n",
    "        for j, (X, y) in tqdm(enumerate(train_iter)):\n",
    "            X[0].to(device), X[1].to(device), y.to(device)\n",
    "            net.zero_grad()\n",
    "            logits = net(X)\n",
    "            L = loss(logits, y)\n",
    "            y_pred = torch.argmax(logits, dim=1)\n",
    "            total_loss.append(L.item())\n",
    "            total_acc.append((y_pred == y).sum() / y.size()[0] * 100)\n",
    "            if j > 1000:\n",
    "                total_loss, total_acc = total_loss[1:], total_acc[1:]\n",
    "            L.backward()\n",
    "            if i < freeze_epoch:\n",
    "                net.freeze_embedding_grad_()\n",
    "            trainer.step()\n",
    "            \n",
    "            if (j+1) % 1000 == 0:\n",
    "                train_loss.append(sum(total_loss) / len(total_loss))\n",
    "                train_acc.append(sum(total_acc) / len(total_acc))\n",
    "                print(f\"iteration {j+1} of epoch {i+1}, train loss:{train_loss[-1]}, train accuracy:{train_acc[-1]}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total_acc = []\n",
    "            for X, y in dev_iter:\n",
    "                X[0].to(device), X[1].to(device), y.to(device)\n",
    "                logits = net(X)\n",
    "                L = loss(logits, y)\n",
    "                y_pred = torch.argmax(logits, dim=1)\n",
    "                total_acc.append((y_pred == y).sum() / y.size()[0] * 100)\n",
    "            dev_acc += [sum(total_acc) / len(total_acc)] * (len(train_iter) // 1000)\n",
    "        \n",
    "        if save:\n",
    "            torch.save(net.state_dict(), PATH + net.name + suffix + '.pth')\n",
    "    return net, (train_loss, train_acc, dev_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedb91b-db3d-4e5e-87e3-4b93ffa16657",
   "metadata": {},
   "source": [
    "**The only difference between Parikh and ESIM lies in the encoder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c065b5c3-dfda-438b-bcfa-a98845dfe89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [01:13, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 of epoch 1, train loss:1.0249824350476264, train accuracy:47.237998962402344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [02:25, 14.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2000 of epoch 1, train loss:0.9416316803637799, train accuracy:55.97502517700195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [03:39, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3000 of epoch 1, train loss:0.9066073506266683, train accuracy:58.50349807739258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4002it [04:57, 12.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4000 of epoch 1, train loss:0.8854592329376823, train accuracy:59.95304870605469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5002it [06:11, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5000 of epoch 1, train loss:0.8699641615360767, train accuracy:60.74725341796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5494it [06:47, 13.47it/s]\n",
      "1001it [01:13, 13.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 of epoch 2, train loss:0.8480161155462265, train accuracy:62.2869987487793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [02:27, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2000 of epoch 2, train loss:0.8432458038334841, train accuracy:62.539459228515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3001it [03:42, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3000 of epoch 2, train loss:0.8367857135140098, train accuracy:62.8211784362793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4002it [04:57, 13.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4000 of epoch 2, train loss:0.8303616816228205, train accuracy:63.22677230834961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5002it [06:11, 12.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5000 of epoch 2, train loss:0.8247710052546445, train accuracy:63.61438751220703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5494it [06:47, 13.48it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding, select = create_embedding(vocab_truncated)\n",
    "parikh = DecomposableAttention(vocab_truncated, 100, 100, embedding, select, embedding.weight.size()[1], 'mlp')\n",
    "num_epochs, lr = 2, 0.001\n",
    "loss = nn.CrossEntropyLoss()\n",
    "trainer = optim.Adam(parikh.parameters(), lr=lr)\n",
    "parikh, history_p = train(parikh, train_iter, dev_iter, loss, trainer, num_epochs, device, freeze_epoch=1, save=True, suffix='-50-30-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93deb942-8f98-491b-9b87-4eb1a36c3ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [13:11,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 of epoch 1, train loss:1.017000095129013, train accuracy:47.78799819946289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [26:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2000 of epoch 1, train loss:0.9204398975386605, train accuracy:56.785213470458984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [38:50,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3000 of epoch 1, train loss:0.8826814089026246, train accuracy:59.72427749633789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [51:41,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4000 of epoch 1, train loss:0.8546697033511532, train accuracy:61.739261627197266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [1:04:32,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5000 of epoch 1, train loss:0.8222182278747444, train accuracy:63.837162017822266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5494it [1:10:53,  1.29it/s]\n",
      "1000it [12:53,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 of epoch 2, train loss:0.7755439891815186, train accuracy:66.50800323486328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [25:45,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2000 of epoch 2, train loss:0.7665508627057909, train accuracy:66.93106842041016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [38:38,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3000 of epoch 2, train loss:0.754696276221242, train accuracy:67.74225616455078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [51:46,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4000 of epoch 2, train loss:0.7479835885030763, train accuracy:68.02198028564453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [1:04:42,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5000 of epoch 2, train loss:0.7436615874121835, train accuracy:68.01798248291016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5494it [1:11:08,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding, select = create_embedding(vocab)\n",
    "esim = DecomposableAttention(vocab, 100, 100, embedding, select, embedding.weight.size()[1], 'bilstm')\n",
    "num_epochs, lr = 2, 0.002\n",
    "loss = nn.CrossEntropyLoss()\n",
    "trainer = optim.Adam(esim.parameters(), lr=lr)\n",
    "esim, history_e = train(esim, train_iter, dev_iter, loss, trainer, num_epochs, device, freeze_epoch=1, save=True, suffix='-50-100-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd1ac72f-c68b-499b-ba35-7cef6da88155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_snli(net, vocab, premise, hypothesis):\n",
    "    net.eval()\n",
    "    premise = torch.tensor(vocab[truncate_pad(premise.rstrip('.').lower().split(), 50, pad='<pad>')])\n",
    "    hypothesis = torch.tensor(vocab[truncate_pad(hypothesis.rstrip('.').lower().split(), 50, pad='<pad>')])\n",
    "    label = torch.argmax(net([premise.reshape((1, -1)),\n",
    "                           hypothesis.reshape((1, -1))]), dim=1)\n",
    "    print(net([premise.reshape((1, -1)), hypothesis.reshape((1, -1))]))\n",
    "    return 'entailment' if label == 0 else 'contradiction' if label == 2 \\\n",
    "            else 'neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4bf25-61c6-4315-96cd-dd8d5bb3579c",
   "metadata": {},
   "source": [
    "**It is questionable because s2 can derive s1 but s1 can't derive s2. These attention model treat the premise and hypothesis equally. I am also curious about whether or not we should \\<pad\\> our setence, because the pooling layer will squeeze the length.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1759dac1-c787-4f93-a103-2147ff72061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1788, -0.3292,  0.2063]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5025, -0.3422,  0.3792]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.2799, -0.2884, -0.1119]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8443, -0.1859,  0.8113]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('contradiction', 'contradiction', 'entailment', 'contradiction')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"I have a driving license.\"\n",
    "s2 = \"I can drive.\"\n",
    "(predict_snli(esim, vocab_truncated, s1, s2), predict_snli(esim, vocab_truncated, s2, s1),\n",
    " predict_snli(parikh, vocab_truncated, s1, s2), predict_snli(parikh, vocab_truncated, s2, s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a83f422-022a-448a-81ce-6e97942c8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7665,  0.0025, -0.2533]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.8471,  0.2624, -0.5545]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.1223,  0.2669, -0.7166]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.2049,  0.4520, -1.1317]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('entailment', 'entailment', 'neutral', 'neutral')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"good\"\n",
    "s2 = \"bad\"\n",
    "(predict_snli(esim, vocab_truncated, s1, s2), predict_snli(esim, vocab_truncated, s2, s1),\n",
    " predict_snli(parikh, vocab_truncated, s1, s2), predict_snli(parikh, vocab_truncated, s2, s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "947d097c-eb2b-4056-b1d5-8d8f2b7aff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1467,  0.0608, -0.0546]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1650,  0.9435, -0.0813]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5233,  0.1290, -0.8376]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1438,  0.8058, -0.2529]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('neutral', 'neutral', 'entailment', 'neutral')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"This church choir sings joyous songs from the book at a church.\"\n",
    "s2 = \"The church is filled with song.\"\n",
    "(predict_snli(esim, vocab_truncated, s1, s2), predict_snli(esim, vocab_truncated, s2, s1),\n",
    " predict_snli(parikh, vocab_truncated, s1, s2), predict_snli(parikh, vocab_truncated, s2, s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffa8c0-d45a-46ec-85f3-bf094cd86aed",
   "metadata": {},
   "source": [
    "**Others**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c088e8a-b053-4699-8943-00c5381f7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, dev_acc = np.array(history_p[0]), np.array(history_p[1])/100, np.array(history_p[2])/100\n",
    "np.save('train_loss_parikh', train_loss)\n",
    "np.save('train_acc_parikh', train_acc)\n",
    "np.save('dev_acc_parikh', dev_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10fb41ae-a91f-4e31-a349-c05a5d851343",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, dev_acc = np.array(history_e[0]), np.array(history_e[1])/100, np.array(history_e[2])/100\n",
    "np.save('train_loss_esim', train_loss)\n",
    "np.save('train_acc_esim', train_acc)\n",
    "np.save('dev_acc_esim', dev_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2cbbe6-8d77-4d92-b965-b65220ff8ed9",
   "metadata": {},
   "source": [
    "**Result on dev_set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cbcba96-338a-4f51-996e-4332638cba29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(65.1785), tensor(69.2530))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_p[-1][-1], history_e[-1][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
